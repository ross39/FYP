\chapter{Technology Review}
In this chapter I will review the technology used in my project.

\subsection{What is machine learning?}
Machine-learning algorithms use statistics to find patterns in signifcant amounts of data. And data, here, encompasses a lot of things such as numbers, words, images, clicks, what have you. If it can be digitally stored, it can be fed into a machine-learning algorithm. Machine learning is behind the scenes of many of the products we use in our daily lives like those on Netflix, YouTube, and Spotify; search engines like Google and Baidu; social-media feeds like Facebook and Twitter; voice assistants like Siri and Alexa. 


\subsection{History of machine learning}
Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM. Machine learning slowly gained traction and the interest of machine learning related to pattern recognition continued during the 1970s, as described in the book of Duda and Hart in 1973.  In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Machine learning grew in popularity due to the scientific endavour to acheieve a general artificial intelligence. As such it was a natural progression that scientists of the day began by trying to have machines learn from data. The techiques they employed were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. 

However there began an increasing emphasis towards a logical, knowlege-based approach which caused a divide between artificial intelligence and machine learning. Systems that employed largely probabilistic approaches were riddled with theoretical and practical problems of data acquisition and representation. This meant that by 1980, expert systems had begun to dominate the field of artifical intelligence and the statistical approach was cast aside. Researchers still continued to work on symbolic/knowlege-based systems which led to inductive logic programming but the more statistical line of research was more cloesly aligned with pattern recognition and information retrieval. Progress was not being made, and as such, research into neural networks had been dropped by the field of artifical intelligence. Much like the statistical approach in the 1980s, research into neural networks continued outside the field of artifical intelligence under the guise of connectionism by researchers from other fields including Hopfield, Rumelhart and Hinton. They made a big discovery in the mid-1980s with the reinvention of backpropagation. Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory. It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.

\subsection{History and technical background of support vector machine(SVM)}
In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.The original SVM algoritm was developed by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. Essentially what this means is that given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). As stated above, in addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.



\subsection{History and technical background of Long Short Term Memory(LSTM) model}
Long Short Term Memory(LSTM) is a type of recurrent neural network(RNN) architecture using in the field of deep learning. LSTM was proposed in 1997 by Sepp Hochreiter and J端rgen Schmidhuber. By introducing Constant Error Carousel (CEC) units, LSTM deals with the exploding and vanishing gradient problems. The initial version of LSTM block included cells, input and output gates. LSTM models are heavily used in industry. Google used LSTM for speech recognition on the smartphone, for the smart assistant Allo and for Google Translate. Apple uses LSTM for the "Quicktype" function on the iPhone and for Siri.Amazon uses LSTM for Amazon Alexa. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs.

\subsection{History and technical background of the Naive Bayes classifier}
In the field of machine learning na誰ve Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (na誰ve) independence assumptions between the features. They are among the simplest Bayesian network models. Na誰ve Bayes has been studied extensively since the 1960s. It was introduced (though not under that name) into the text retrieval community in the early 1960s,[2] and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (document categorization)(such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis.


\subsection{History and technical background of the Tensorflow and Keras frameworks}
\subsubsection{Tensorflow}
TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache License 2.0 on November 9, 2015. Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications.[5][7] Google assigned multiple computer scientists, including Jeff Dean, to simplify and refactor the codebase of DistBelief into a faster, more robust application-grade library, which became TensorFlow.[8] In 2009, the team, led by Geoffrey Hinton, had implemented generalized backpropagation and other improvements which allowed generation of neural networks with substantially higher accuracy, for instance a 25\% reduction in errors in speech recognition.

TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors. Its flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. The release of tensorflow made the implementation of machine learning models much more attainable. Use of tensorflow to implement machine learning models increased rapidly upon it's release so much so that during the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google

\subsection{History and technical background of Apache Kafka}